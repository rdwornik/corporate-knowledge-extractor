# CLAUDE.md - Corporate Knowledge Extractor

## Project Overview

Python pipeline that extracts structured knowledge from corporate meeting recordings (MP4/MKV) with PowerPoint slides. Produces markdown reports with slide images and JSONL Q&A knowledge base for RAG systems.

**Primary Use Case:** After a meeting, generate educational notes that capture not just what's on slides, but what the speaker EXPLAINED about them - the nuances, context, and insider knowledge.

**Current State:** Optimized for Type A meetings (training videos with slides). Types B and C are on the roadmap.

## Meeting Types

The project is designed to support three types of corporate meetings:

### Type A: Internal Training Sessions (CURRENT FOCUS)
**Characteristics:**
- Video recordings with PowerPoint presentations
- Presenter explaining slides with detailed commentary
- Educational content (product training, technical overviews)
- 30-90 minute duration typical
- Large file sizes (2-4GB common)

**Pipeline Requirements:**
- Frame extraction (slide detection)
- OCR (reading slide text)
- Transcription (what speaker said)
- Alignment (match speech to slides)
- Knowledge synthesis (extract insights from speech)

**Output Focus:**
- Detailed educational reports
- Q&A pairs for knowledge base
- Categorized by technical topics

### Type B: Client Meetings (ROADMAP)
**Characteristics:**
- Often audio-only (no presentation)
- Discussion-based, not slide-driven
- Focus on decisions, action items, concerns
- Multiple speakers
- 30-60 minute duration typical

**Pipeline Requirements:**
- Transcription with speaker diarization
- Action item detection
- Decision tracking
- Sentiment analysis
- No frame extraction needed

**Output Focus:**
- Meeting summary
- Action items with owners
- Client concerns/requests
- Follow-up tasks

### Type C: Internal Updates (ROADMAP)
**Characteristics:**
- Mix of formats (video, audio, shared documents)
- Status updates, roadmap reviews
- Cross-functional impact
- What changed, what's new
- Variable duration

**Pipeline Requirements:**
- Multi-format ingestion (video, audio, PDF, DOCX)
- Change detection
- Impact analysis
- Task extraction

**Output Focus:**
- Change summary
- Impact by team/function
- Action items
- Timeline/roadmap updates

## Tech Stack

- **Transcription:** Whisper via Groq API (fast, cheap)
- **Frame Extraction:** OpenCV with pixel-based change detection
- **OCR:** Tesseract (local)
- **Semantic Tagging:** Gemini Flash API (batch processing)
- **Knowledge Synthesis:** Gemini Flash API (chunked processing)
- **Anonymization:** spaCy NER + custom terms
- **Output:** Markdown report + JSONL Q&A pairs

## Project Structure

```
corporate-knowledge-extractor/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â””â”€â”€ knowledge_extraction.txt    # LLM prompt template
â”‚   â”œâ”€â”€ settings.yaml                   # Main configuration
â”‚   â”œâ”€â”€ processing.yaml                 # Frame/alignment settings
â”‚   â”œâ”€â”€ anonymization.yaml              # Redaction terms
â”‚   â”œâ”€â”€ categories.yaml                 # Report categories
â”‚   â”œâ”€â”€ filters.yaml                    # Junk/filler patterns
â”‚   â””â”€â”€ config_loader.py                # Config loading utility
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ transcribe/
â”‚   â”‚   â””â”€â”€ groq_backend.py             # Whisper transcription
â”‚   â”œâ”€â”€ frames/
â”‚   â”‚   â”œâ”€â”€ extractor.py                # Frame extraction + deduplication
â”‚   â”‚   â””â”€â”€ tagger.py                   # Semantic tagging via LLM
â”‚   â”œâ”€â”€ ocr/
â”‚   â”‚   â””â”€â”€ reader.py                   # Tesseract OCR
â”‚   â”œâ”€â”€ align/
â”‚   â”‚   â””â”€â”€ aligner.py                  # Speech-to-frame alignment
â”‚   â”œâ”€â”€ anonymize/
â”‚   â”‚   â””â”€â”€ anonymizer.py               # PII redaction
â”‚   â”œâ”€â”€ synthesize/
â”‚   â”‚   â”œâ”€â”€ base.py                     # Base synthesizer class
â”‚   â”‚   â””â”€â”€ gemini_backend.py           # Gemini API integration
â”‚   â””â”€â”€ output/
â”‚       â”œâ”€â”€ generator.py                # Markdown/JSONL generation
â”‚       â””â”€â”€ post_processor.py           # Deduplication, categorization
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run.py                          # Main entry point
â”œâ”€â”€ data/
â”‚   â””â”€â”€ input/                          # Place video files here
â””â”€â”€ output/                             # Generated reports
```

## Pipeline Flow

```
Video File
    â†“
[1] transcribe_groq() â†’ segments[] {start, end, text}
    â†“
[2] extract_frames() â†’ frames[] {timestamp, path}
    â†“
[3] read_frames() â†’ frames[] + {text} (OCR)
    â†“
[4] tag_frames() â†’ frames[] + {tags} (semantic)
    â†“
[5] align() â†’ aligned[] {start, end, speech, slide_text, frame_idx}
    â†“
[6] anonymize() â†’ redacted aligned[] and frames[]
    â†“
[7] synthesize() â†’ {slide_breakdown[], qa_pairs[]}
    â†“
[8] post_process() â†’ deduplicated, categorized synthesis
    â†“
[9] generate_output() â†’ report.md + knowledge.jsonl
```

## Critical Architecture Decisions

### Frame-Speech Alignment
The synthesizer receives BOTH `frames[]` AND `aligned_data[]`:
- `frames[]` = actual PNG images sorted by timestamp
- `aligned_data[]` = transcript segments with speech content

Speech is mapped to frames by timestamp range, NOT by OCR text grouping. This ensures frame images match their descriptions in the report.

### FRAME_ID Consistency
Each frame gets a unique ID (001, 002, 003...) that flows through:
1. Synthesizer prompt â†’ LLM output
2. Generator â†’ image filenames
3. Markdown report â†’ image references

**Never renumber frames mid-pipeline.**

### LLM Output Fields
The LLM produces these fields (defined in knowledge_extraction.txt):
- `frame_id`: String "001", "002", etc.
- `title`: Descriptive title
- `visual_content`: What's shown on slide
- `technical_details`: Numbers, versions, specs
- `speaker_explanation`: **MAIN CONTENT** - what speaker said
- `context_relationships`: Connections to other topics
- `key_terminology`: Term definitions

**The generator MUST use `speaker_explanation`** - this contains the educational value.

## Configuration Files

### settings.yaml
```yaml
input:
  directory: "data/input"
  video_extensions: [".mp4", ".mkv", ".avi", ".mov"]

llm:
  model: "gemini-2.0-flash"
  chunk_size: 10
```

### anonymization.yaml
```yaml
custom_terms:      # Always redact these
  - "Blue Yonder"
  - "ClientName"

exclude_terms:     # Never redact (products misidentified as persons)
  - "WMS"
  - "BYDM"
```

### categories.yaml
```yaml
order: [infrastructure, sla, api, architecture, security, ...]

titles:
  infrastructure: "ğŸ—ï¸ Infrastructure & Platform"
  sla: "ğŸ“‹ Service Level Agreements"
  
keywords:
  infrastructure: [saas, platform, data center, azure]
  sla: [sla, availability, disaster recovery, rto]
```

## Common Issues & Solutions

### Problem: Frame images don't match text descriptions
**Cause:** Mismatch between frame numbering systems
**Solution:** Ensure both synthesizer and generator sort frames by timestamp before assigning IDs

### Problem: Report has empty/generic descriptions
**Cause:** Generator looking for wrong field (e.g., `key_insight` instead of `speaker_explanation`)
**Solution:** Check generator.py `_format_slide()` uses fields that LLM actually produces

### Problem: Too many duplicate slides
**Cause:** Similar OCR text creates near-duplicate frames
**Solution:** Adjust `dedup_similarity` threshold in processing.yaml, or improve post_processor deduplication

### Problem: Speaker explanations are generic summaries
**Cause:** Prompt not emphasizing extraction of actual speech content
**Solution:** Prompt must instruct LLM to capture WHAT speaker said, not summarize THAT they spoke

## Recent Architecture Changes

### Configuration Refactoring (Dec 2025)
Migrated from hardcoded values to centralized YAML configuration:

**Before:** Magic numbers and paths scattered across 9 Python modules
**After:** Domain-separated YAML files with single responsibility

**New config structure:**
- `settings.yaml` - Paths, file extensions, LLM models, tool paths
- `processing.yaml` - Frame extraction, deduplication, alignment parameters
- `anonymization.yaml` - Custom terms, exclusions, auto-detection settings
- `categories.yaml` - Categorization rules and keywords
- `filters.yaml` - Junk patterns, filler content, stop words

**config_loader.py features:**
- Dot-notation access: `get("processing", "frames.sample_rate")`
- In-memory caching for performance
- Path resolution: `get_path("settings", "input.directory")`
- Graceful defaults: `get("settings", "llm.model", "gemini-2.0-flash")`

**Benefits:**
- Zero hardcoded values in source code
- Easy configuration changes without code modifications
- Improved testability and deployment flexibility
- Clear separation between configuration and implementation

## Development Guidelines

### Adding New Config Values
1. Identify the appropriate YAML file (settings/processing/filters/etc.)
2. Add with descriptive comment
3. Update config_loader.py if new file needed
4. Update consuming code to use `get("file", "key.path", default)`
5. Never hardcode values - always use config_loader

### Modifying LLM Prompt
1. Work on `feature/prompt-optimization` branch
2. Make ONE change at a time
3. Test with full pipeline run
4. Compare report quality before/after
5. Commit with descriptive message

### Git Workflow
- `main`: Stable, working code
- `feature/*`: Experimental changes
- Commit after EVERY working change
- Never mix prompt changes with code changes in same commit

## Environment Setup

```bash
# Required
pip install opencv-python pytesseract spacy python-dotenv google-genai

# spaCy model
python -m spacy download en_core_web_sm

# Tesseract (Windows)
# Install from: https://github.com/UB-Mannheim/tesseract/wiki
# Add to PATH: C:\Program Files\Tesseract-OCR

# .env file
GEMINI_API_KEY=your_key_here
GROQ_API_KEY=your_key_here
```

## Running

```bash
# Place video in data/input/
python scripts/run.py

# Output in output/YYYY-MM-DD_HHMM/
#   - report.md
#   - knowledge.jsonl
#   - frames/
#   - metadata.json
```

## Cost Estimate

Per 50-minute meeting:
- Groq API (Whisper): Free tier / ~$0.001
- Gemini API (tagging + synthesis): ~$0.01
- **Total: ~$0.01-0.02**

## Testing & Quality Assurance

### Automated Quality Checks
The `tests/test_quality.py` module provides automated report quality validation:

**check_speaker_explanation_quality():**
- Ensures speaker_explanation field is populated
- Detects raw transcript dumps (red flag)
- Validates educational value extraction
- Min length threshold: 30 chars (configurable)

**check_no_junk_frames():**
- Verifies junk filter patterns work
- Detects "loading", "thank you", generic slides
- Ensures only valuable content in report

**check_categories_balanced():**
- Prevents everything dumping into "general"
- Validates categorization keywords working
- Reports category distribution

**check_qa_pairs_quality():**
- Validates Q&A format (question/answer fields)
- Checks for specificity (not generic)
- Ensures category tagging
- Verifies source frame references

### Manual Review Checklist
Before deploying updated prompts or configs:
1. Process sample training video (known good baseline)
2. Run automated quality tests
3. Manual spot-check:
   - Do frame images match descriptions?
   - Are speaker explanations detailed (not summaries)?
   - Are technical details specific (not generic)?
   - Is PII properly anonymized?
4. Compare metrics to baseline (frames, Q&A count, categories)
5. Git commit with test results in message

### Known Quality Issues
1. **Duplicate slides:** OCR similarity can create near-duplicates
   - Mitigation: Post-processor deduplication
   - Tuning: `processing.yaml` dedup_similarity threshold

2. **Generic explanations:** LLM sometimes summarizes instead of extracting
   - Mitigation: Prompt emphasizes "what speaker SAID"
   - Detection: Automated quality check flags short/generic text

3. **Frame numbering mismatch:** Rare bug where images don't match text
   - Root cause: Sorting inconsistency between synthesizer and generator
   - Both must sort frames by timestamp before ID assignment
   - Detection: Manual verification during spot checks

## Content Type Presets System

### Architecture

The preset system allows customizing frame extraction behavior for different content types (PowerPoint, Excel, Demo, Audio-only, Hybrid).

**Location:** `config/presets/*.yaml`

**Components:**
1. **Preset YAML files** - Configuration for each content type
2. **extractor.py** - `load_preset()` function loads YAML into dict
3. **run.py** - CLI argument `--preset <name>` passes to extractor
4. **AdaptiveFrameTracker** - Class for hybrid mode dynamic switching

### Available Presets

**powerpoint.yaml:**
- Default behavior, optimized for slide-based presentations
- Sample rate: 1s, Pixel threshold: 5%, Max/min: 10
- Use case: Training videos, pitch decks

**excel.yaml:**
- Sparse sampling for spreadsheet scrolling
- Sample rate: 10s, Pixel threshold: 30%, Max/min: 3
- Prevents frame explosion from cell selection/scroll
- Use case: Financial reports, dashboard reviews

**demo.yaml:**
- Time-based sampling for software demonstrations
- Sample rate: 15s, Pixel threshold: 25%, Max/min: 4
- Ignores cursor movement and hover states
- Use case: Feature walkthroughs, UI tutorials

**audio_only.yaml:**
- Disables frame extraction (`frames.enabled: false`)
- Returns empty frame list, skips OCR/tagging/alignment
- Use case: Client calls, phone meetings, interviews

**hybrid.yaml:**
- Adaptive mode with automatic content detection
- Analyzes activity every 60 seconds (`analysis_window`)
- Switches between powerpoint/demo modes based on frame rate
- Logs mode switches for transparency
- Use case: Mixed content (slides + demo + discussion)

### How Presets Work

**1. Loading:**
```python
from src.frames.extractor import load_preset

preset_config = load_preset("excel")  # Loads config/presets/excel.yaml
# Returns dict with:
# - frames: {sample_rate, pixel_threshold, max_per_minute, max_total}
# - deduplication: {enabled, similarity_threshold, pixel_similarity}
# - synthesis: {focus, speaker_explanation_weight}
```

**2. Applying:**
```python
extract_frames(
    video_path,
    preset="excel",  # Uses excel.yaml settings
    sample_rate=None,  # Override if needed
    threshold=None
)
```

**3. Adaptive Mode (Hybrid Preset):**
```python
class AdaptiveFrameTracker:
    # Tracks frame activity in sliding window
    def add_frame(timestamp)
    def should_check_switch(current_time)
    def check_and_switch(total_frames, current_time)
        # Analyzes frames/minute in last 60 seconds
        # High activity (>5 f/min) â†’ switch to demo mode
        # Low activity (<2 f/min) â†’ switch to powerpoint mode
        # Returns new settings dict or None
```

**4. Usage:**
```bash
# Via CLI
python scripts/run.py --preset excel

# Via code
f = extract_frames("video.mp4", preset="excel")
```

### Adding New Presets

1. **Create YAML file** in `config/presets/`:
```yaml
name: "My Custom Preset"
description: "Description of use case"

frames:
  enabled: true
  sample_rate: 5  # Seconds between checks
  pixel_threshold: 0.20  # 0.0-1.0
  max_per_minute: 8
  max_total: 200

  deduplication:
    enabled: true
    similarity_threshold: 0.88
    pixel_similarity: 0.88

synthesis:
  focus: "my_focus_type"
  speaker_explanation_weight: "high"
```

2. **Update run.py** - Add to `choices` list in argparse
3. **Update README.md** - Document the new preset
4. **Test** - Verify with representative video

### Preset Configuration Fields

**frames.enabled:** Boolean - Enable/disable frame extraction
**frames.sample_rate:** Int - Seconds between frame checks
**frames.pixel_threshold:** Float 0.0-1.0 - % of pixels changed = new frame
**frames.max_per_minute:** Int - Prevent frame explosion
**frames.max_total:** Int - Hard limit for very long videos
**frames.mode:** String - "adaptive" for hybrid, omit for static
**frames.deduplication:** Dict - Similarity thresholds for dedup

**Adaptive mode fields (hybrid only):**
**frames.analysis_window:** Int - Seconds in sliding window (60)
**frames.adaptive_rules:** Dict - Thresholds for mode switching
**frames.modes:** Dict - Settings for each sub-mode (powerpoint/demo/excel)

### Design Decisions

**Why presets vs. hardcoded values?**
- Different content types need different strategies
- Excel scrolling generates 10x more frames than PowerPoint
- Demo cursor movement shouldn't trigger frames
- Audio-only needs to skip frame pipeline entirely

**Why YAML vs. Python?**
- Non-technical users can adjust settings
- No code changes needed for tuning
- Version control shows config history
- Easy A/B testing of parameters

**Why adaptive mode?**
- Real meetings mix content types
- Manual mode switching is tedious
- Auto-detection handles 80% of cases
- Logging shows when switches happen (transparency)

## Report Comparison System

### Architecture

The comparison system detects quality regressions by diff'ing two reports.

**Location:** `scripts/compare_reports.py`

**Components:**
1. **load_report_data()** - Loads markdown, JSONL, metadata
2. **compare_*()** functions - Frame/slide/QA/quality comparisons
3. **determine_verdict()** - Overall assessment (improved/degraded/mixed)
4. **generate_markdown_report()** - Human-readable diff
5. **generate_json_metrics()** - Machine-readable metrics

### How It Works

**1. Load Reports:**
```python
old = load_report_data("output/2025-01-01_1200")
new = load_report_data("output/2025-01-03_1430")

# Extracts:
# - Slide titles and explanations from markdown
# - Q&A pairs from knowledge.jsonl
# - Frame count from frames/ directory
# - Quality metrics via QualityChecker
```

**2. Compare Metrics:**
```python
comparison = {
    "frames": {"old_count": 94, "new_count": 87, "change": -7},
    "slides": {"removed_titles": [...], "added_titles": [...]},
    "qa_pairs": {"old_count": 280, "new_count": 310, "change": +30},
    "quality": {
        "improvements": ["Longer explanations (+20%)"],
        "regressions": []
    }
}
```

**3. Determine Verdict:**
```python
def determine_verdict(comparison):
    if regressions and not improvements:
        return {"verdict": "degraded", "has_regressions": True}
    elif improvements and not regressions:
        return {"verdict": "improved", "has_regressions": False}
    elif improvements and regressions:
        return {"verdict": "mixed", "has_regressions": True}
    else:
        return {"verdict": "unchanged", "has_regressions": False}
```

**4. Generate Outputs:**
- **comparison_report.md** - Summary table, improvements/regressions lists, changed explanations
- **comparison_metrics.json** - All metrics for CI/CD integration

### Usage

**Basic Comparison:**
```bash
python scripts/compare_reports.py output/old output/new
# Outputs: comparison_report.md + comparison_metrics.json
```

**CI/CD Integration:**
```bash
# Fail build if regressions detected
python scripts/compare_reports.py \
  tests/fixtures/baseline \
  output/latest \
  --fail-on-regression

# Exit codes:
# 0 = no regressions (pass)
# 1 = regressions detected (fail)
```

**Baseline Workflow:**
```bash
# 1. Establish baseline
python scripts/run.py
cp -r output/latest tests/fixtures/baseline_v1

# 2. Make changes (update prompt, config, code)
# ... edit files ...

# 3. Re-run pipeline
python scripts/run.py

# 4. Compare
python scripts/compare_reports.py \
  tests/fixtures/baseline_v1 \
  output/latest

# 5. Review comparison_report.md
# 6. If improved: update baseline
# 7. If degraded: fix issues
```

### Comparison Metrics

**Frames:**
- Count change (absolute and percentage)
- Indicates if frame extraction improved/degraded

**Slides:**
- Removed titles (slides that disappeared)
- Added titles (new slides detected)
- Helps identify if deduplication changed

**Q&A Pairs:**
- Count change (more Q&A = more knowledge extracted)
- Percentage change

**Quality Metrics:**
- Average explanation length (longer = more detail)
- Empty explanation count (should be low)
- Junk slide count (should decrease over time)
- General category percentage (should decrease = better categorization)

**Content Changes:**
- Slides with changed explanations
- Change type: improved/degraded/rewritten
- Length comparison

### Design Decisions

**Why compare reports vs. raw pipeline output?**
- Reports are the user-facing artifact
- Markdown diffs are human-readable
- Quality metrics in reports reflect actual value

**Why separate markdown and JSON outputs?**
- Markdown for humans (review in GitHub, email)
- JSON for machines (CI/CD, automated alerts)

**Why fail-on-regression flag?**
- Prevents accidental quality degradation
- Blocks PR merges if tests show regression
- Forces explicit override for intentional changes

**Why track removed/added slides?**
- Deduplication changes can remove valid content
- New slides indicate better detection
- Helps debug frame extraction issues

### Integration with Quality Tests

```python
# tests/test_quality.py provides the metrics
from tests.test_quality import QualityChecker

checker = QualityChecker(report_dir)
quality_data = {
    "speaker_explanation": checker.check_speaker_explanation_quality(),
    "junk_frames": checker.check_no_junk_frames(),
    "categories": checker.check_categories_balanced(),
    "qa_pairs": checker.check_qa_pairs_quality()
}

# compare_reports.py uses these metrics for comparison
```

## Future Improvements

1. **Multi-input support (Type B/C meetings):**
   - Audio-only processing (skip frame extraction)
   - Document ingestion (PDF, DOCX, XLSX)
   - Auto-detect meeting type

2. **Quality improvements:**
   - Prompt optimization for better insight extraction
   - Smarter duplicate detection (semantic, not just OCR)
   - A/B testing framework for prompt changes

3. **Operational:**
   - Video compression before processing (4GB â†’ 500MB)
   - Incremental processing (skip already-processed files)
   - CI/CD with automated quality gates

4. **Features:**
   - Multi-language support (non-English meetings)
   - Speaker diarization (who said what)
   - Web UI (upload video, download report)
   - Batch processing dashboard

## Git

Commit frequently with clear messages. Push after each working feature.

## Audio Preprocessing for Large Files

### Problem

Groq API has a 25MB file size limit for transcription. However, corporate meeting recordings (especially 5-hour training sessions) typically produce large audio files:

- **Raw video**: 2-4GB (typical corporate meeting)
- **Extracted audio (compressed)**: 50-80MB (5-hour meeting at 32kbps)
- **After silence removal**: 30-50MB (30-40% reduction)
- **After chunking**: Multiple 24MB chunks

### Solution Architecture

The transcription system uses a 3-stage preprocessing pipeline:

```
Input Video (3GB)
    â†“
[1] Extract Audio â†’ 73MB MP3 (mono, 16kHz, 32kbps)
    â†“
[2] Remove Silence â†’ 45MB MP3 (38% reduction)
    â†“
[3] Split Chunks â†’ 2x 24MB chunks
    â†“
[4] Transcribe â†’ 2 separate API calls
    â†“
[5] Merge Transcripts â†’ Single timeline with corrected timestamps
```

### Stage 1: Audio Extraction & Optimization

**Module:** `src/transcribe/groq_backend.py::extract_audio()`

Extracts audio from video and applies Whisper-optimized settings:

```python
# FFmpeg optimization
-vn             # No video
-ac 1           # Mono (stereo not needed for speech)
-ar 16000       # 16kHz sample rate (Whisper optimal)
-b:a 32k        # 32kbps bitrate (sufficient for speech)
```

**Typical Results:**
- 2GB MP4 â†’ 45MB MP3 (98% reduction)
- Quality: Perfect for Whisper transcription
- No loss of speech intelligibility

### Stage 2: Silence Removal

**Module:** `scripts/preprocess_audio.py::remove_silence()`

Uses FFmpeg `silenceremove` filter to eliminate long pauses while preserving natural speech timing:

```python
# FFmpeg silenceremove filter
silenceremove=
    start_periods=1:               # Remove silence from start
    start_threshold=-40dB:         # Silence = audio below -40dB
    stop_periods=-1:               # Remove all silence
    stop_duration=2.0:             # Only remove pauses > 2 seconds
    detection=peak                 # Peak detection method
```

**Configuration (settings.yaml):**
```yaml
transcription:
  silence_removal:
    enabled: true
    threshold_db: -40          # -30 = aggressive, -50 = conservative
    min_silence_duration: 2.0  # Remove pauses > 2 seconds
```

**Threshold Guidelines:**
- `-30dB`: Aggressive (removes more, risk of clipping quiet speech)
- `-40dB`: **Recommended** (good balance for meetings)
- `-50dB`: Conservative (keeps more content)

**Typical Results:**
- 73MB audio with 40% silence â†’ 45MB (38% reduction)
- 5-hour recording â†’ 3-hour effective speech time
- Preserves all speech content, removes long pauses between topics

**When Silence Removal Helps Most:**
- Training sessions with long pauses
- Presentations with slide transition gaps
- Recordings with dead air at start/end
- Meetings with extended silences

**When to Disable:**
- Music/audio with intentional pauses
- Recordings where timing is critical
- Already heavily edited audio

### Stage 3: Audio Chunking

**Module:** `src/transcribe/chunker.py`

When silence removal alone isn't enough to get below 25MB, the audio is split into chunks.

**Key Features:**

1. **Intelligent Boundary Detection:**
   - Uses pydub to detect silence boundaries
   - Splits at natural pauses (not mid-word)
   - Avoids cutting sentences

2. **Overlap for Context:**
   - 5-second overlap between chunks (configurable)
   - Prevents loss of words at boundaries
   - Overlap segments filtered during merge

3. **Size-Based Splitting:**
   ```python
   # Calculate number of chunks needed
   num_chunks = ceil(file_size_mb / max_chunk_size_mb)

   # Target chunk duration
   target_duration = total_duration / num_chunks

   # Find silence boundary closest to target
   split_point = find_nearest_silence_boundary(target_duration)
   ```

**Example:**
- 45MB file, 24MB max â†’ 2 chunks (22.5MB each)
- Chunk 1: 0:00-2:30 (with 5s extension to 2:35)
- Chunk 2: 2:30-5:00 (starts at 2:25 due to overlap)

### Stage 4: Transcript Merging

**Module:** `src/transcribe/chunker.py::merge_transcripts()`

After transcribing each chunk separately, transcripts are merged with timeline corrections:

```python
# Chunk 1 segments (0:00-2:35)
[
    {"start": 0, "end": 5, "text": "Welcome to the training"},
    {"start": 5, "end": 150, "text": "Let's begin..."},
    {"start": 150, "end": 155, "text": "Next slide"}  # Overlap
]

# Chunk 2 segments (originally 0:00-2:30, offset by 150s)
[
    {"start": 0, "end": 5, "text": "Next slide"},  # SKIP (overlap)
    {"start": 5, "end": 150, "text": "Now we'll cover..."}
]

# Merged result
[
    {"start": 0, "end": 5, "text": "Welcome to the training"},
    {"start": 5, "end": 150, "text": "Let's begin..."},
    {"start": 150, "end": 155, "text": "Next slide"},
    {"start": 155, "end": 300, "text": "Now we'll cover..."}  # Offset applied
]
```

**Merging Logic:**
1. Process chunks sequentially
2. Apply cumulative time offset to each segment
3. Skip segments in overlap region (first N seconds of each chunk)
4. Maintain continuous timeline

### Configuration

**config/settings.yaml:**
```yaml
transcription:
  provider: "groq"
  max_file_size_mb: 25

  silence_removal:
    enabled: true
    threshold_db: -40
    min_silence_duration: 2.0

  chunking:
    enabled: true
    max_chunk_size_mb: 24
    overlap_seconds: 5.0
```

### Usage Examples

**Automatic Preprocessing (Default):**
```python
from src.transcribe.groq_backend import transcribe_groq

# Automatically handles large files
segments = transcribe_groq("5_hour_training.mp4")
# â†’ Extracts audio, removes silence, chunks if needed, merges
```

**Manual Preprocessing:**
```python
from scripts.preprocess_audio import preprocess_for_transcription

# Preprocess separately
output, stats = preprocess_for_transcription(
    "training.mp4",
    remove_silence_enabled=True,
    threshold_db=-40,
    min_silence_duration=2.0
)

print(f"Reduced from {stats['original_size_mb']:.1f}MB to {stats['final_size_mb']:.1f}MB")
# â†’ "Reduced from 73.2MB to 44.8MB"
```

**Disable Preprocessing:**
```python
# For small files or when preprocessing not needed
segments = transcribe_groq(
    "short_meeting.mp4",
    enable_preprocessing=False,
    enable_chunking=False
)
```

### Size Estimates

**Typical 5-Hour Corporate Training:**
- Original video: 3.5GB MP4
- Extracted audio: 73MB MP3 (32kbps, mono, 16kHz)
- After silence removal: 45MB (38% reduction)
- Chunks needed: 2 chunks of 22.5MB each
- API calls: 2 (one per chunk)
- Processing time: ~8-12 minutes (Groq API)
- Cost: Free tier / ~$0.002

**Size Reduction by Meeting Type:**

| Meeting Type | Original | After Silence | Reduction | Reason |
|--------------|----------|---------------|-----------|--------|
| Training (5h) | 73MB | 45MB | 38% | Long pauses between slides |
| Demo (2h) | 30MB | 22MB | 27% | Some pauses, mostly continuous |
| Discussion (1h) | 15MB | 13MB | 13% | Continuous speech, few pauses |
| Webinar (3h) | 50MB | 28MB | 44% | Q&A pauses, intro/outro silence |

**Rule of Thumb:**
- Presentations with slides: 30-40% reduction
- Software demos: 20-30% reduction
- Discussions/meetings: 10-20% reduction
- Webinars with Q&A: 40-50% reduction

### Troubleshooting

**Problem: File still too large after silence removal**
```bash
# Check file size
python -c "import os; print(f'{os.path.getsize('audio.mp3')/1024/1024:.1f}MB')"

# If > 25MB, chunking will automatically activate
# Verify chunking is enabled in settings.yaml
```

**Problem: Speech getting cut off**
```yaml
# Adjust silence threshold (more conservative)
transcription:
  silence_removal:
    threshold_db: -50  # Was -40, now keeps more content
    min_silence_duration: 3.0  # Was 2.0, now only removes longer pauses
```

**Problem: Too many chunks**
```yaml
# Increase chunk size (closer to limit)
transcription:
  chunking:
    max_chunk_size_mb: 24.5  # Was 24, now uses more headroom
```

**Problem: Words lost at chunk boundaries**
```yaml
# Increase overlap
transcription:
  chunking:
    overlap_seconds: 10.0  # Was 5.0, now more context preserved
```

### Testing

Run transcription tests:
```bash
pytest tests/test_transcription.py -v
```

Tests cover:
- Silence removal reduces file size
- Speech content preserved after silence removal
- Audio chunking at correct boundaries
- Transcript merging with correct timestamps
- Overlap handling
- Full preprocessing pipeline

### Performance Metrics

**Processing Time (5-hour meeting):**
1. Extract audio: ~30 seconds (FFmpeg)
2. Remove silence: ~45 seconds (FFmpeg)
3. Split chunks: ~15 seconds (pydub)
4. Transcribe chunk 1: ~3 minutes (Groq API)
5. Transcribe chunk 2: ~3 minutes (Groq API)
6. Merge transcripts: <1 second (Python)

**Total: ~7-8 minutes** (vs. hours with local Whisper)

**API Cost (Groq):**
- 45MB audio = 2 chunks
- 2 API calls Ã— $0.001 = $0.002
- **Effectively free** on Groq's free tier

## Model Selection Guide

Use `/model claude-sonnet-4-20250514` (default) for:
- Creating simple files and functions
- Small edits, quick fixes
- Running tests and commands
- Iterative development
- Simple CRUD operations

Use `/model claude-opus-4-20250514` for:
- System architecture decisions
- Complex debugging (errors spanning multiple files)
- Refactoring across multiple files
- Large context analysis (understanding whole codebase)
- Code review and optimization
- When Sonnet fails 2+ times on same task

Rule: Start with Sonnet. Switch to Opus when stuck or task is complex.